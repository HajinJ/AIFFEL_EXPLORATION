{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34cb2543",
      "metadata": {
        "id": "34cb2543",
        "outputId": "a4974f3e-3edc-42b9-f7f2-33132dc99178"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train data 개수: 150000, test data 개수: 50000\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9976970</td>\n",
              "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3819312</td>\n",
              "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10265843</td>\n",
              "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9045019</td>\n",
              "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6483659</td>\n",
              "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id                                           document  label\n",
              "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
              "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
              "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
              "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
              "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import urllib.request\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from konlpy.tag import Okt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from collections import Counter\n",
        "\n",
        "# 데이터 파일 읽어오기\n",
        "train_data = pd.read_table('~/aiffel/sentiment_classification/data/ratings_train.txt')\n",
        "test_data = pd.read_table('~/aiffel/sentiment_classification/data/ratings_test.txt')\n",
        "\n",
        "print(\"train data 개수: {}, test data 개수: {}\".format(len(train_data), len(test_data)))  # 데이터 개수 확인\n",
        "train_data.head()  # 데이터 출력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8089fe1f",
      "metadata": {
        "id": "8089fe1f",
        "outputId": "1f99fb53-e2b6-4276-f463-8737b8c9e418"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gensim==3.8.3\n",
            "  Downloading gensim-3.8.3.tar.gz (23.4 MB)\n",
            "     |████████████████████████████████| 23.4 MB 7.5 MB/s            \n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (1.21.4)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (1.7.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (1.16.0)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (5.2.1)\n",
            "Building wheels for collected packages: gensim\n",
            "  Building wheel for gensim (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for gensim: filename=gensim-3.8.3-cp39-cp39-linux_x86_64.whl size=24328218 sha256=5494aa42a00074a93028d124e229e2b13ca9eff6ab31149c65e15fb6c1c5882d\n",
            "  Stored in directory: /aiffel/.cache/pip/wheels/ca/5d/af/618594ec2f28608c1d6ee7d2b7e95a3e9b06551e3b80a491d6\n",
            "Successfully built gensim\n",
            "Installing collected packages: gensim\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 4.1.2\n",
            "    Uninstalling gensim-4.1.2:\n",
            "      Successfully uninstalled gensim-4.1.2\n",
            "Successfully installed gensim-3.8.3\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade gensim==3.8.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "078e24d3",
      "metadata": {
        "id": "078e24d3",
        "outputId": "fc7ba523-1e61-4eef-ba93-e85d90e3257c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1364/3300053641.py:9: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")  # 특수문자 제거\n",
            "/tmp/ipykernel_1364/3300053641.py:14: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")  # 특수문자 제거\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train data 개수: 145791, test data 개수: 48995\n"
          ]
        }
      ],
      "source": [
        "from konlpy.tag import Mecab\n",
        "tokenizer = Mecab()\n",
        "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다'] # 불용어\n",
        "\n",
        "# 데이터 로더 함수 정의\n",
        "def load_data(train_data, test_data, num_words=10000):\n",
        "    # train data 전처리\n",
        "    train_data.drop_duplicates(subset=['document'], inplace=True)  # 중복 제거\n",
        "    train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")  # 특수문자 제거\n",
        "    train_data['document'].replace('', np.nan, inplace=True)  # 공백은 Null로 변경\n",
        "    train_data = train_data.dropna(how = 'any')  # 결측치 제거\n",
        "    # test data 전처리\n",
        "    test_data.drop_duplicates(subset=['document'], inplace=True)  # 중복 제거\n",
        "    test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")  # 특수문자 제거\n",
        "    test_data['document'].replace('', np.nan, inplace=True)  # 공백은 Null로 변경\n",
        "    test_data = test_data.dropna(how = 'any')  # 결측치 제거\n",
        "\n",
        "    X_train = []\n",
        "    for sentence in train_data['document']:\n",
        "        temp_X = tokenizer.morphs(sentence) # 토큰화\n",
        "        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
        "        X_train.append(temp_X)\n",
        "\n",
        "    X_test = []\n",
        "    for sentence in test_data['document']:\n",
        "        temp_X = tokenizer.morphs(sentence) # 토큰화\n",
        "        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
        "        X_test.append(temp_X)\n",
        "    \n",
        "    # 단어사전 만들기\n",
        "    words = np.concatenate(X_train).tolist()\n",
        "    counter = Counter(words)\n",
        "    counter = counter.most_common(10000-4)  # 단어 빈도순으로 (10000-4)개 가져오기\n",
        "    vocab = ['<PAD>', '<BOS>', '<UNK>', '<UNUSED>'] + [key for key, _ in counter]  # 앞부분 4개 추가\n",
        "    word_to_index = {word:index for index, word in enumerate(vocab)}  # {단어:인덱스} 단어사전 생성\n",
        "    \n",
        "    # 리뷰 텍스트를 단어사전 인덱스로 변환\n",
        "    def wordlist_to_indexlist(wordlist):\n",
        "        return [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in wordlist]\n",
        "    \n",
        "    X_train = list(map(wordlist_to_indexlist, X_train))\n",
        "    X_test = list(map(wordlist_to_indexlist, X_test))\n",
        "\n",
        "    return X_train, np.array(list(train_data['label'])), X_test, np.array(list(test_data['label'])), word_to_index\n",
        "\n",
        "# 데이터 로더 실행하여 데이터셋 생성\n",
        "X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data)\n",
        "print(\"train data 개수: {}, test data 개수: {}\".format(len(X_train), len(X_test)))\n",
        "\n",
        "index_to_word = {index:word for word, index in word_to_index.items()}  # {인덱스:단어} 딕셔너리 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dee118d",
      "metadata": {
        "id": "4dee118d",
        "outputId": "1b2d1d7d-c2a6-4611-a476-06ed1a3874c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[27, 67, 895, 33, 214, 15, 28, 699]\n",
            "더 빙 진짜 짜증 나 네요 목소리\n",
            "라벨:  0\n",
            "[977, 481, 491, 636, 4, 110, 1554, 48, 866, 949, 11, 38, 364]\n",
            "포스터 보고 초딩 영화 줄 오버 연기 조차 가볍 지 않 구나\n",
            "라벨:  1\n",
            "[19, 192, 2]\n",
            "재 <UNK>\n",
            "라벨:  0\n",
            "[8035, 143, 4134, 277, 85, 13, 5, 52, 3326]\n",
            "이야기 구먼 솔직히 재미 없 다 평점 조정\n",
            "라벨:  0\n",
            "[2, 8488, 1051, 48, 2702, 54, 4, 2625, 34, 1118, 29, 326, 36, 17, 35, 54, 2, 2, 393, 2850, 1677, 5]\n",
            "익살 스런 연기 돋보였 던 영화 스파이더맨 에서 늙 어 보이 기 만 했 던 <UNK> <UNK> 너무나 이뻐 보였 다\n",
            "라벨:  1\n",
            "[627, 2, 2, 425, 171, 1464, 661, 1592, 665, 46, 2, 4, 61, 2167, 118, 718]\n",
            "<UNK> <UNK> 세 부터 초등 학교 학년 생 인 <UNK> 영화 ㅋㅋㅋ 별반 개 아까움\n",
            "라벨:  0\n",
            "[237, 351, 8, 354, 1909, 51, 11, 780, 5]\n",
            "긴장감 을 제대로 살려 내 지 못했 다\n",
            "라벨:  0\n",
            "[232, 1344, 132, 5, 392, 705, 2, 2, 48, 1434, 269, 93, 233, 23, 882, 24, 321, 608, 501, 475, 546, 5, 3165, 8489, 17, 1386, 1386, 65, 282, 13, 5, 48, 95, 7, 70, 17, 2, 37]\n",
            "반개 아깝 다 욕 나온다 <UNK> <UNK> 연기 생활 몇 년 인지 정말 발 로 해도 그것 보단 낫 겟 다 납치 감금 만 반복 반복 드라마 가족 없 다 연기 못 하 사람 만 <UNK> 네\n",
            "라벨:  0\n",
            "[117, 13, 18, 85, 12, 269, 22, 43, 4]\n",
            "없 는데 재미 있 몇 안 되 영화\n",
            "라벨:  1\n",
            "[57, 612, 52, 225, 706, 531, 81, 17, 478, 1403, 596, 688, 74, 17, 19, 2, 12, 15]\n",
            "케 평점 낮 건데 꽤 볼 만 한데 헐리우드 식 화려 함 만 너무 <UNK> 있 나\n",
            "라벨:  1\n"
          ]
        }
      ],
      "source": [
        "# 인코딩 & 디코딩 함수 정의\n",
        "# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트 벡터로 변환해 주는 함수입니다. \n",
        "# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \n",
        "def get_encoded_sentence(sentence, word_to_index):\n",
        "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
        "\n",
        "# 여러 개의 문장 리스트를 한꺼번에 단어 인덱스 리스트 벡터로 encode해 주는 함수입니다. \n",
        "def get_encoded_sentences(sentences, word_to_index):\n",
        "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
        "\n",
        "# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \n",
        "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
        "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
        "\n",
        "# 여러개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다. \n",
        "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
        "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]\n",
        "\n",
        "# decode된 문장과 라벨을 비교하여 일치하는지 확인\n",
        "for i in range(10):\n",
        "    print(X_train[i])\n",
        "    print(get_decoded_sentence(X_train[i], index_to_word))\n",
        "    print('라벨: ', y_train[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc9ad1f4",
      "metadata": {
        "id": "cc9ad1f4",
        "outputId": "1434c7c7-0d66-478c-8603-88b928b47830"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "문장길이 평균 :  13.757179674103888\n",
            "문장길이 최대 :  83\n",
            "문장길이 표준편차 :  11.462771769216866\n",
            "pad_sequences maxlen :  36\n",
            "전체 문장의 93.38145451931864%가 maxlen 설정값 이내에 포함됩니다. \n"
          ]
        }
      ],
      "source": [
        "# 텍스트 데이터 문장 길이의 리스트 생성\n",
        "total_data_text = list(X_train) + list(X_test)\n",
        "num_tokens = [len(tokens) for tokens in total_data_text]\n",
        "num_tokens = np.array(num_tokens)\n",
        "# 문장 길이의 평균, 최대값, 표준편차 계산\n",
        "print('문장길이 평균 : ', np.mean(num_tokens))\n",
        "print('문장길이 최대 : ', np.max(num_tokens))\n",
        "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
        "\n",
        "# 최대 길이를 (평균 + 2*표준편차)로 계산\n",
        "max_tokens = np.mean(num_tokens) + 2*np.std(num_tokens)\n",
        "maxlen = int(max_tokens)\n",
        "print('pad_sequences maxlen : ', maxlen)\n",
        "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens) / len(num_tokens) *100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4450763",
      "metadata": {
        "id": "b4450763",
        "outputId": "5b68ca78-fb7f-4b7c-b9ec-8e8ba73c158d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(145791, 36)\n"
          ]
        }
      ],
      "source": [
        "# padding으로 문장 길이 맞추기\n",
        "X_train = keras.preprocessing.sequence.pad_sequences(X_train,\n",
        "                                       value=word_to_index[\"<PAD>\"],\n",
        "                                       padding='post',\n",
        "                                       maxlen=maxlen)\n",
        "\n",
        "X_test = keras.preprocessing.sequence.pad_sequences(X_test,\n",
        "                                       value=word_to_index[\"<PAD>\"],\n",
        "                                       padding='post',\n",
        "                                       maxlen=maxlen)\n",
        "\n",
        "print(X_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55a37c81",
      "metadata": {
        "id": "55a37c81",
        "outputId": "41b3e230-9f9e-47b3-f8ff-263e2f02a59b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(105791, 36)\n",
            "(105791,)\n"
          ]
        }
      ],
      "source": [
        "# 훈련용 데이터셋 145791건 중 40000건을 분리하여 validation set으로 사용\n",
        "x_val = X_train[:40000]\n",
        "y_val = y_train[:40000]\n",
        "\n",
        "# validation set을 제외한 나머지는 train set으로 사용\n",
        "partial_x_train = X_train[40000:]\n",
        "partial_y_train = y_train[40000:]\n",
        "\n",
        "print(partial_x_train.shape)\n",
        "print(partial_y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c6d5229",
      "metadata": {
        "id": "5c6d5229",
        "outputId": "7fbb6c19-5d78-4456-c124-df9aa325f0cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 16)          160000    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 8)                 800       \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 160,881\n",
            "Trainable params: 160,881\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "vocab_size = 10000    # 어휘 사전의 크기(10,000개의 단어)\n",
        "word_vector_dim = 16  # 단어 하나를 표현하는 임베딩 벡터의 차원수 (변경가능)\n",
        "\n",
        "# LSTM 레이어로 모델 설계\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
        "model.add(keras.layers.LSTM(8))  # LSTM state 벡터의 차원수 (변경가능)\n",
        "model.add(keras.layers.Dense(8, activation='relu'))\n",
        "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52889d0c",
      "metadata": {
        "id": "52889d0c",
        "outputId": "a349a28f-b0a3-45db-9fe9-9d82065a1d04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "207/207 [==============================] - 5s 8ms/step - loss: 0.6374 - accuracy: 0.5997 - val_loss: 0.4489 - val_accuracy: 0.8159\n",
            "Epoch 2/5\n",
            "207/207 [==============================] - 1s 6ms/step - loss: 0.3904 - accuracy: 0.8348 - val_loss: 0.3663 - val_accuracy: 0.8404\n",
            "Epoch 3/5\n",
            "207/207 [==============================] - 1s 6ms/step - loss: 0.3390 - accuracy: 0.8577 - val_loss: 0.3599 - val_accuracy: 0.8422\n",
            "Epoch 4/5\n",
            "207/207 [==============================] - 1s 5ms/step - loss: 0.3219 - accuracy: 0.8661 - val_loss: 0.3609 - val_accuracy: 0.8411\n",
            "Epoch 5/5\n",
            "207/207 [==============================] - 1s 6ms/step - loss: 0.3133 - accuracy: 0.8693 - val_loss: 0.3660 - val_accuracy: 0.8401\n"
          ]
        }
      ],
      "source": [
        "# model 학습\n",
        "model.compile(optimizer='adam',\n",
        "             loss='binary_crossentropy',\n",
        "             metrics=['accuracy'])\n",
        "            \n",
        "epochs=5\n",
        "\n",
        "history = model.fit(partial_x_train,\n",
        "                   partial_y_train,\n",
        "                   epochs=epochs,\n",
        "                   batch_size=512,\n",
        "                   validation_data=(x_val, y_val),\n",
        "                   verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b2f9d25",
      "metadata": {
        "id": "3b2f9d25",
        "outputId": "07cd5054-257f-4e3f-9587-cb18bd82887a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1532/1532 - 3s - loss: 0.3742 - accuracy: 0.8365\n",
            "[0.374218225479126, 0.8364934921264648]\n"
          ]
        }
      ],
      "source": [
        "# test set으로 model 평가\n",
        "results = model.evaluate(X_test,  y_test, verbose=2)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a8265b9",
      "metadata": {
        "id": "4a8265b9",
        "outputId": "b27c3bf1-8be5-4161-c832-327659d4f238"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 16)          160000    \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, None, 16)          784       \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, None, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, None, 16)          784       \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 161,713\n",
            "Trainable params: 161,713\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "vocab_size = 10000    # 어휘 사전의 크기(10,000개의 단어)\n",
        "word_vector_dim = 16  # 단어 하나를 표현하는 임베딩 벡터의 차원수 (변경가능)\n",
        "\n",
        "# 1-D CNN 모델 설계\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
        "model.add(keras.layers.Conv1D(16, 3, activation='relu'))\n",
        "model.add(keras.layers.MaxPooling1D(5))\n",
        "model.add(keras.layers.Conv1D(16, 3, activation='relu'))\n",
        "model.add(keras.layers.GlobalMaxPooling1D())\n",
        "model.add(keras.layers.Dense(8, activation='relu'))\n",
        "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0703e0f0",
      "metadata": {
        "id": "0703e0f0",
        "outputId": "1be6f1d0-3c68-4e3a-e4b0-ced5c9d9199c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "207/207 [==============================] - 3s 6ms/step - loss: 0.5160 - accuracy: 0.7382 - val_loss: 0.3757 - val_accuracy: 0.8317\n",
            "Epoch 2/5\n",
            "207/207 [==============================] - 1s 5ms/step - loss: 0.3427 - accuracy: 0.8531 - val_loss: 0.3578 - val_accuracy: 0.8428\n",
            "Epoch 3/5\n",
            "207/207 [==============================] - 1s 5ms/step - loss: 0.3029 - accuracy: 0.8731 - val_loss: 0.3567 - val_accuracy: 0.8444\n",
            "Epoch 4/5\n",
            "207/207 [==============================] - 1s 5ms/step - loss: 0.2729 - accuracy: 0.8888 - val_loss: 0.3645 - val_accuracy: 0.8446\n",
            "Epoch 5/5\n",
            "207/207 [==============================] - 1s 5ms/step - loss: 0.2448 - accuracy: 0.9038 - val_loss: 0.3806 - val_accuracy: 0.8405\n"
          ]
        }
      ],
      "source": [
        "# model 학습\n",
        "model.compile(optimizer='adam',\n",
        "             loss='binary_crossentropy',\n",
        "             metrics=['accuracy'])\n",
        "            \n",
        "epochs=5\n",
        "\n",
        "history = model.fit(partial_x_train,\n",
        "                   partial_y_train,\n",
        "                   epochs=epochs,\n",
        "                   batch_size=512,\n",
        "                   validation_data=(x_val, y_val),\n",
        "                   verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "946a652a",
      "metadata": {
        "id": "946a652a",
        "outputId": "a79b3657-ebaf-4045-fae0-35f7ce6c1f5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1532/1532 - 2s - loss: 0.3858 - accuracy: 0.8382\n",
            "[0.38579079508781433, 0.838167130947113]\n"
          ]
        }
      ],
      "source": [
        "# test set으로 model 평가\n",
        "results = model.evaluate(X_test,  y_test, verbose=2)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dad8226d",
      "metadata": {
        "id": "dad8226d",
        "outputId": "05306edf-9ab1-4f24-db6a-6f454b091ddc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, None, 16)          160000    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_1 (Glob (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 160,145\n",
            "Trainable params: 160,145\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "vocab_size = 10000    # 어휘 사전의 크기(10,000개의 단어)\n",
        "word_vector_dim = 16  # 단어 하나를 표현하는 임베딩 벡터의 차원수 (변경가능)\n",
        "\n",
        "# GlobalMaxPooling1D() 레이어 하나만 사용하여 모델 설계\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
        "model.add(keras.layers.GlobalMaxPooling1D())\n",
        "model.add(keras.layers.Dense(8, activation='relu'))\n",
        "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d342a75",
      "metadata": {
        "id": "8d342a75",
        "outputId": "a5337abf-1d87-4f65-e055-dbb294534a09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "207/207 [==============================] - 1s 4ms/step - loss: 0.6038 - accuracy: 0.7512 - val_loss: 0.4669 - val_accuracy: 0.8084\n",
            "Epoch 2/5\n",
            "207/207 [==============================] - 1s 4ms/step - loss: 0.4047 - accuracy: 0.8301 - val_loss: 0.3828 - val_accuracy: 0.8296\n",
            "Epoch 3/5\n",
            "207/207 [==============================] - 1s 4ms/step - loss: 0.3473 - accuracy: 0.8529 - val_loss: 0.3673 - val_accuracy: 0.8353\n",
            "Epoch 4/5\n",
            "207/207 [==============================] - 1s 4ms/step - loss: 0.3170 - accuracy: 0.8677 - val_loss: 0.3634 - val_accuracy: 0.8380\n",
            "Epoch 5/5\n",
            "207/207 [==============================] - 1s 4ms/step - loss: 0.2946 - accuracy: 0.8787 - val_loss: 0.3661 - val_accuracy: 0.8386\n"
          ]
        }
      ],
      "source": [
        "# model 학습\n",
        "model.compile(optimizer='adam',\n",
        "             loss='binary_crossentropy',\n",
        "             metrics=['accuracy'])\n",
        "            \n",
        "epochs=5\n",
        "\n",
        "history = model.fit(partial_x_train,\n",
        "                   partial_y_train,\n",
        "                   epochs=epochs,\n",
        "                   batch_size=512,\n",
        "                   validation_data=(x_val, y_val),\n",
        "                   verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbc29344",
      "metadata": {
        "id": "bbc29344",
        "outputId": "b8dedc4d-17bf-43f8-b231-e2232d808aaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1532/1532 - 2s - loss: 0.3736 - accuracy: 0.8370\n",
            "[0.37364456057548523, 0.8370242118835449]\n"
          ]
        }
      ],
      "source": [
        "# test set으로 model 평가\n",
        "results = model.evaluate(X_test,  y_test, verbose=2)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "004e9d66",
      "metadata": {
        "id": "004e9d66",
        "outputId": "ac8dcabf-589d-4d76-b5f4-21b16f39859b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10000, 16)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import gensim\n",
        "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
        "from tensorflow.keras.initializers import Constant\n",
        "\n",
        "# 임베딩 레이어 차원 확인\n",
        "embedding_layer = model.layers[0]\n",
        "weights = embedding_layer.get_weights()[0]\n",
        "print(weights.shape)    # shape: (vocab_size, embedding_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8bc4d9a",
      "metadata": {
        "id": "a8bc4d9a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# 학습한 Embedding 파라미터를 파일에 써서 저장\n",
        "word2vec_file_path = os.getenv('HOME')+'/aiffel/sentiment_classification/word2vec.txt'\n",
        "f = open(word2vec_file_path, 'w')\n",
        "f.write('{} {}\\n'.format(vocab_size-4, word_vector_dim))  # 몇개의 벡터를 얼마 사이즈로 기재할지 타이틀을 씁니다.\n",
        "\n",
        "# 단어 개수(특수문자 4개는 제외)만큼의 워드 벡터를 파일에 기록\n",
        "vectors = model.get_weights()[0]\n",
        "for i in range(4,vocab_size):\n",
        "    f.write('{} {}\\n'.format(index_to_word[i], ' '.join(map(str, list(vectors[i, :])))))\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d14b4375",
      "metadata": {
        "id": "d14b4375",
        "outputId": "98398ba5-5f3e-43d8-b036-f8444863e779"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('구토', 0.9171332120895386),\n",
              " ('시작', 0.9100662469863892),\n",
              " ('제일', 0.9050424098968506),\n",
              " ('오', 0.8893713355064392),\n",
              " ('안소니', 0.8870009183883667),\n",
              " ('동생', 0.8856402039527893),\n",
              " ('봉태규', 0.8831815719604492),\n",
              " ('활동', 0.880566418170929),\n",
              " ('배우', 0.8726174235343933),\n",
              " ('자화상', 0.8700385093688965)]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
        "# 파일에 기록된 임베딩 파라미터를 읽어서 word vector로 활용\n",
        "word_vectors = Word2VecKeyedVectors.load_word2vec_format(word2vec_file_path, binary=False)\n",
        "\n",
        "# 유사한 단어와 그 유사도 확인\n",
        "word_vectors.similar_by_word(\"재미\")  # 학습이 잘 되지 않아 별로 유사하지 않음"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cf24091",
      "metadata": {
        "id": "2cf24091",
        "outputId": "48157d51-6274-4e05-8de4-9527a7fc504c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1364/2946491341.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  vector = word2vec['영화']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(200,)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "word2vec_path = os.getenv('HOME')+'/aiffel/sentiment_classification/ko.bin'\n",
        "word2vec = gensim.models.Word2Vec.load(word2vec_path)\n",
        "vector = word2vec['영화']\n",
        "vector.shape     # 200dim의 워드 벡터"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ac32447",
      "metadata": {
        "id": "4ac32447",
        "outputId": "4a4bf2f4-406d-4278-a60f-d44e62d8fe1b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('취미', 0.5857348442077637),\n",
              " ('유머', 0.5140613913536072),\n",
              " ('매력', 0.5105490684509277),\n",
              " ('흥미', 0.4988338351249695),\n",
              " ('공짜', 0.4960595667362213),\n",
              " ('일자리', 0.49294644594192505),\n",
              " ('즐거움', 0.48700767755508423),\n",
              " ('비애', 0.4836210310459137),\n",
              " ('관객', 0.48286449909210205),\n",
              " ('향수', 0.4823310971260071)]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 유사한 단어와 그 유사도 확인\n",
        "word2vec.wv.most_similar(\"재미\")  # 학습이 잘 되어 유사함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffb36a87",
      "metadata": {
        "id": "ffb36a87",
        "outputId": "ea594e3f-810b-4cc3-c020-b4a858595825"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1364/1410859879.py:10: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "  if index_to_word[i] in word2vec:\n",
            "/tmp/ipykernel_1364/1410859879.py:11: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  embedding_matrix[i] = word2vec[index_to_word[i]]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.initializers import Constant\n",
        "\n",
        "vocab_size = 10000     # 어휘 사전의 크기(10,000개의 단어)\n",
        "word_vector_dim = 200  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
        "\n",
        "embedding_matrix = np.random.rand(vocab_size, word_vector_dim)\n",
        "\n",
        "# embedding_matrix에 Word2Vec 워드벡터를 단어 하나씩 차례대로 카피\n",
        "for i in range(4,vocab_size):\n",
        "    if index_to_word[i] in word2vec:\n",
        "        embedding_matrix[i] = word2vec[index_to_word[i]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b78eb7ce",
      "metadata": {
        "id": "b78eb7ce",
        "outputId": "f9c41b6c-089b-47c8-f77b-c9a9d2b9bc65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 36, 200)           2000000   \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 128)               168448    \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 2,168,577\n",
            "Trainable params: 2,168,577\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# 모델 구성\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Embedding(vocab_size, \n",
        "                                 word_vector_dim, \n",
        "                                 embeddings_initializer=Constant(embedding_matrix),  # 카피한 임베딩을 여기서 활용\n",
        "                                 input_length=maxlen, \n",
        "                                 trainable=True))   # trainable을 True로 주면 Fine-tuning\n",
        "model.add(keras.layers.LSTM(128))\n",
        "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a408dcdf",
      "metadata": {
        "id": "a408dcdf",
        "outputId": "567bd3bb-7a7a-4c2d-9b6a-c73df8153fc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1653/1653 [==============================] - 11s 6ms/step - loss: 0.4468 - accuracy: 0.7726 - val_loss: 0.3557 - val_accuracy: 0.8426\n",
            "Epoch 2/5\n",
            "1653/1653 [==============================] - 10s 6ms/step - loss: 0.3101 - accuracy: 0.8649 - val_loss: 0.3316 - val_accuracy: 0.8550\n",
            "Epoch 3/5\n",
            "1653/1653 [==============================] - 10s 6ms/step - loss: 0.2647 - accuracy: 0.8877 - val_loss: 0.3335 - val_accuracy: 0.8539\n",
            "Epoch 4/5\n",
            "1653/1653 [==============================] - 9s 6ms/step - loss: 0.2241 - accuracy: 0.9070 - val_loss: 0.3515 - val_accuracy: 0.8520\n",
            "Epoch 5/5\n",
            "1653/1653 [==============================] - 10s 6ms/step - loss: 0.1852 - accuracy: 0.9253 - val_loss: 0.3804 - val_accuracy: 0.8529\n"
          ]
        }
      ],
      "source": [
        "# 학습 진행\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "              \n",
        "epochs=5 # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
        "\n",
        "history = model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=epochs,\n",
        "                    batch_size=64,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e1d03e5",
      "metadata": {
        "id": "4e1d03e5",
        "outputId": "d8ae585e-bfbc-4894-b7d2-0e9defd91254"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1532/1532 - 4s - loss: 0.3832 - accuracy: 0.8510\n",
            "[0.38318902254104614, 0.8509848117828369]\n"
          ]
        }
      ],
      "source": [
        "# 테스트셋을 통한 모델 평가\n",
        "results = model.evaluate(X_test,  y_test, verbose=2)\n",
        "print(results)   # 정확도가 0.85으로 개선됨"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 프로젝트 정리\n",
        "- LSTM, 1-D CNN, GlobalMaxPooling1D 3가지 모델로 학습을 시켜봄. \n",
        "정확도는 모두 85% 이하. \n",
        "- gensim의 유사단어 찾기를 통해 학습한 결과 '재미'라는 단어로 비교했을 때, 자체학습한 임베딩은 재미와 관련 없는 단어들이 나와 의미가 없다는걸 확인, word2vec은 '유머', '흥미', '즐거움' 등 유사도가 높은 단어가 나와 사전학습이 잘 이루어진 것을 확인할 수 있었다.\n",
        "- 사전학습된 word2vec을 사용하면 모델의 성능이 많이 개선될 것이라 기대했지만, 실제로는 정확도가 85%로 조금밖에 향상되지 않았다. 한국어의 구조가 복잡하고 형태소 분석까지 추가되기 때문인거 같음. 더불어, 네이버 영화리뷰 데이터 자체에 잘못된 라벨이나 내가 모르는 다른 데이터 종류가 포함되어 있을 가능성도 있는 것 같다"
      ],
      "metadata": {
        "id": "Y1X5MhWt_NjJ"
      },
      "id": "Y1X5MhWt_NjJ"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "EXPLORATION_6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}